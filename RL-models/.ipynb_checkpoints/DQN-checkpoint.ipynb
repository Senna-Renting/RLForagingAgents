{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959697fd-16aa-43ac-98e8-a9d798d17019",
   "metadata": {},
   "source": [
    "# DQN implementation in JAX\n",
    "\n",
    "I will implement the DQN algorithm as proposed by the following paper [1]. I will try to make the model as general as possible, so that it can be used for any type of problem. The model will be implemented in JAX, so that fast training and testing will be possible with the model.\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] M Roderick et al. 2017, Implementing the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "458892b3-9e00-4ecf-ba57-2ba5c9ed4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn import relu\n",
    "from jax import grad, value_and_grad, jit\n",
    "from jax.random import normal, PRNGKey\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65923641-3b93-49f0-b647-5c2f86643f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network hard-coded version for Inverted Pendulum case\n",
    "\n",
    "# class InvPendulumNN:\n",
    "#     def __init__(self, in_size=5, out_size=1, hidden_size=10, a_func=relu, seed=42):\n",
    "#         prng_key = PRNGKey(seed)\n",
    "#         self.W1 = normal(prng_key,shape=(hidden_size,in_size))\n",
    "#         self.W2 = normal(prng_key,shape=(out_size,hidden_size))\n",
    "#         self.params = {\"W1\": self.W1, \"W2\": self.W2}\n",
    "#         self.dW1 = jnp.zeros_like(self.W1)\n",
    "#         self.dW2 = jnp.zeros_like(self.W2)\n",
    "#         self.a_funcs = [a_func, a_func]\n",
    "#     def predict_loss(self, params, x):\n",
    "#         x_h = self.a_funcs[0](params[\"W1\"] @ x)\n",
    "#         return self.a_funcs[1](params[\"W2\"] @ x_h)\n",
    "#     def predict(self, x):\n",
    "#         x_h = self.a_funcs[0](self.params[\"W1\"] @ x)\n",
    "#         return self.a_funcs[1](self.params[\"W2\"] @ x_h)\n",
    "#     def loss(self, params, batch):\n",
    "#         x,y = batch\n",
    "#         return jnp.square(y - self.predict_loss(params, x)).mean()\n",
    "#     def parameters(self):\n",
    "#         return self.params\n",
    "#     def set_parameters(self, params):\n",
    "#         self.params = params\n",
    "\n",
    "prng_key = PRNGKey(42)\n",
    "\n",
    "class InvPendulumNNv2:\n",
    "    def predict(params, x):\n",
    "        x_h = relu(params[\"W1\"] @ x)\n",
    "        return relu(params[\"W2\"] @ x_h)\n",
    "    def loss(params, batch):\n",
    "        x,y = batch\n",
    "        return jnp.square(y - InvPendulumNNv2.predict(params, x)).mean()\n",
    "    def generate_params(in_size=5, hidden_size=10, out_size=1):\n",
    "        return {\n",
    "            \"W1\":normal(prng_key,shape=(hidden_size,in_size)),\n",
    "            \"W2\":normal(prng_key,shape=(out_size,hidden_size))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fef11409-2c3a-496f-a5b2-bfde05f0fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell block contains the general update mechanics for the parameters of a model\n",
    "def update(parameters, batch, opt_state, optimizer=optax.adam, loss_func=InvPendulumNNv2.loss):\n",
    "    params = parameters\n",
    "    loss, grads = value_and_grad(loss_func)(params, batch)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "# Assumes the following data structure: data = (num_batches, num_samples, batch_size)\n",
    "partial(jit, static_argnums=(3,4))\n",
    "def train(X_train, y_train, params, lr, n_epochs, optimizer=optax.adam):\n",
    "    optimizer = optimizer(lr)\n",
    "    for e in range(n_epochs):\n",
    "        opt_state = optimizer.init(params)\n",
    "        for i in range(X_train.shape[0]):\n",
    "            params, opt_state = update(params, (X_train[i, :, :], y_train[i, :]), opt_state, optimizer=optimizer)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d637598f-b9b3-4b95-b27d-c28efba2ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 1)\n",
      "(1, 1, 1)\n",
      "[[[9.671792]]]\n"
     ]
    }
   ],
   "source": [
    "# Example to test the train method\n",
    "X_train = jnp.array([[[1.], [2.], [3.], [4.], [-5.]]])\n",
    "y_train = jnp.array([[[10.]]])\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "params = InvPendulumNNv2.generate_params(hidden_size=21)\n",
    "params = train(X_train, y_train, params, 2e-3, 30)\n",
    "print(InvPendulumNNv2.predict(params, X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72910601-bdb8-4a7f-a7d3-9550c9c40ec4",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Okay so far I've figured out how to use the gradient method of jax as well as how to use the parameters and an optimizer to update the model iteratively over episodes. Next step will be to check it's performance one sample data from the OpenAI gym model. We will sample an episode of 500 steps in the environment, and use the retrieved data, to check if we can learn Q-values with this neural network. If we are able to do so, the next step will be to actually implement the DQN algorithm fully, with the neural network as a component of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2190b-5fb0-4582-bb18-1e8a0dcfacb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
